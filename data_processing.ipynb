{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc90a99",
   "metadata": {},
   "source": [
    "# Loading, processing, and resaving 2015-2021 data for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab6b23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "import math\n",
    "\n",
    "import datetime as dt\n",
    "import matplotlib.dates\n",
    "\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib.dates import MonthLocator, DateFormatter\n",
    "from matplotlib import *\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e91694d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full</th>\n",
       "      <th>Spaced</th>\n",
       "      <th>Key</th>\n",
       "      <th>Key2</th>\n",
       "      <th>Key3</th>\n",
       "      <th>Key4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alipur, Delhi - DPCC</td>\n",
       "      <td>Alipur</td>\n",
       "      <td>Alipur</td>\n",
       "      <td>Alipur</td>\n",
       "      <td>Alipur</td>\n",
       "      <td>Alipur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anand Vihar, Delhi - DPCC</td>\n",
       "      <td>Anand Vihar</td>\n",
       "      <td>Anand</td>\n",
       "      <td>AnandVihar</td>\n",
       "      <td>AnandVihar</td>\n",
       "      <td>AnandVihar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashok Vihar, Delhi - DPCC</td>\n",
       "      <td>Ashok Vihar</td>\n",
       "      <td>Ashok</td>\n",
       "      <td>AshokVihar</td>\n",
       "      <td>AshokVihar</td>\n",
       "      <td>AshokVihar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aya Nagar, Delhi - IMD</td>\n",
       "      <td>Aya Nagar</td>\n",
       "      <td>Aya</td>\n",
       "      <td>AyaNagar</td>\n",
       "      <td>AyaNagar</td>\n",
       "      <td>AyaNagar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bawana, Delhi - DPCC</td>\n",
       "      <td>Bawana</td>\n",
       "      <td>Bawana</td>\n",
       "      <td>Bawana</td>\n",
       "      <td>Bawana</td>\n",
       "      <td>Bawana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Full       Spaced     Key        Key2        Key3  \\\n",
       "0       Alipur, Delhi - DPCC       Alipur  Alipur      Alipur      Alipur   \n",
       "1  Anand Vihar, Delhi - DPCC  Anand Vihar   Anand  AnandVihar  AnandVihar   \n",
       "2  Ashok Vihar, Delhi - DPCC  Ashok Vihar   Ashok  AshokVihar  AshokVihar   \n",
       "3     Aya Nagar, Delhi - IMD    Aya Nagar     Aya    AyaNagar    AyaNagar   \n",
       "4       Bawana, Delhi - DPCC       Bawana  Bawana      Bawana      Bawana   \n",
       "\n",
       "         Key4  \n",
       "0      Alipur  \n",
       "1  AnandVihar  \n",
       "2  AshokVihar  \n",
       "3    AyaNagar  \n",
       "4      Bawana  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of stations\n",
    "stations_file = '~/Documents/GitHub/HMEI-2021/stations.csv'\n",
    "\n",
    "df_stations = pd.read_csv(stations_file)\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686564a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer to stations by preset keys\n",
    "stations20_21 = [s for s in df_stations['Key3'] if type(s) == str]\n",
    "stations15_19 = [s for s in df_stations['Key4'] if type(s) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb7ea9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(stations20_21))\n",
    "print(len(stations15_19))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768991e4",
   "metadata": {},
   "source": [
    "# Load and join 2015-19 data with 2020-21 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4e223eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that each date corresponds to the correct index\n",
    "# valid if nothing is printed\n",
    "def checkDates(dates, indices, new):\n",
    "    # if new, check only the \"new\" stations, those not in the 2015-19 set\n",
    "    if new:\n",
    "        stations = [x for x in stations20_21 if x not in stations15_19]\n",
    "    else:\n",
    "        stations = stations15_19 # only check stations in the 2015-19 set\n",
    "    for station in stations:\n",
    "        printed = False\n",
    "        if station_data[station].index[indices[0]] != pd.to_datetime(dates[0]):\n",
    "            print(station)\n",
    "            printed = True\n",
    "            print(station_data[station].loc[dates[0]])\n",
    "        for i in range(1, len(dates)):\n",
    "            if station_data[station].index[indices[i]] != pd.to_datetime(dates[i]):\n",
    "                if not printed:\n",
    "                    print(station)\n",
    "                    printed = True\n",
    "                print(station_data[station].loc[dates[i]])\n",
    "        if printed:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "922c9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pollutants = ['PM2.5', 'PM10', 'NO2', 'SO2', 'Ozone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9594a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data = {} # dictionary to store each station's data\n",
    "\n",
    "# load data from Jan, 2015-Nov, 2019\n",
    "for station in stations15_19:\n",
    "    try:\n",
    "        file_str = '~/Documents/GitHub/HMEI-2021/Data 2015-19/data_' + station + '.csv'\n",
    "        df_station = pd.read_csv(file_str, nrows=43345)\n",
    "        \n",
    "        # set datetime as index\n",
    "        df_station.index = pd.to_datetime(df_station['From Date'])\n",
    "        \n",
    "        # reorder columns uniformly\n",
    "        cols = [p for p in pollutants if p in df_station.columns]\n",
    "        df_station = df_station[cols]\n",
    "        \n",
    "        end_date = pd.to_datetime('2019-11-30 23:00:00')\n",
    "        df_station = df_station.loc[:end_date]\n",
    "        \n",
    "        for col in df_station.columns:\n",
    "            df_station[col] = pd.to_numeric(df_station[col], errors='coerce')\n",
    "        \n",
    "        station_data[station] = df_station\n",
    "        \n",
    "    except:\n",
    "        print(station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1f1391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkDates(['2015-01-01 00:00:00', '2019-11-30 23:00:00'], [0, -1], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "672616ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for Dec, 2019\n",
    "for station in stations15_19:\n",
    "    try:\n",
    "        file_str = '~/Documents/GitHub/HMEI-2021/Data 2015-19/Missing Data/data_' + station + '.xlsx'\n",
    "        df_station = pd.read_excel(file_str, skiprows=16)\n",
    "        \n",
    "        # set datetime as index\n",
    "        df_station.index = pd.to_datetime(df_station['From Date'])\n",
    "        \n",
    "        # reorder columns uniformly\n",
    "        cols = [p for p in pollutants if p in df_station.columns]\n",
    "        df_station = df_station[cols]\n",
    "        \n",
    "        end_date = pd.to_datetime('2019-12-31 23:00:00')\n",
    "        df_station = df_station.loc[:end_date]\n",
    "        \n",
    "        for col in df_station.columns:\n",
    "            df_station[col] = pd.to_numeric(df_station[col], errors='coerce')\n",
    "        \n",
    "        station_data[station] = station_data[station].append(df_station)\n",
    "        \n",
    "    except:\n",
    "        print(station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cacf763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkDates(['2015-01-01 00:00:00', '2019-12-31 23:00:00'], [0, -1], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb3366df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in stations20_21:\n",
    "    try:\n",
    "        file_str = '~/Documents/GitHub/HMEI-2021/Data 2020-21/data_' + station + '.xlsx'\n",
    "        df_station = pd.read_excel(file_str, skiprows=16)\n",
    "\n",
    "        # set datetime as index\n",
    "        df_station.index = pd.to_datetime(df_station['From Date'])\n",
    "\n",
    "        cols = [p for p in pollutants if p in df_station.columns]\n",
    "        df_station = df_station[cols]\n",
    "        \n",
    "        end_date = pd.to_datetime('2021-05-31 23:45:00')\n",
    "        df_station = df_station.loc[:end_date]\n",
    "\n",
    "        # convert data to numeric\n",
    "        for col in df_station.columns:\n",
    "            df_station[col] = pd.to_numeric(df_station[col], errors='coerce')\n",
    "\n",
    "        hourly = df_station.resample('H').mean().loc[:'2021-05-31']\n",
    "\n",
    "        if station in stations15_19:\n",
    "            station_data[station] = station_data[station].append(hourly)\n",
    "        else:\n",
    "            station_data[station] = hourly\n",
    "            \n",
    "    except:\n",
    "        print(station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f3cbebd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkDates(['2015-01-01 00:00:00', '2021-05-31 23:00:00', '2019-12-31 23:00:00', '2020-01-01 00:00:00'], \n",
    "           [0, -1, 43823, 43824], False)\n",
    "checkDates(['2020-01-01 00:00:00', '2021-05-31 23:00:00'], [0, -1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "652bac74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 4)\n",
      "(56232, 5)\n",
      "(56232, 4)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 4)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 4)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 4)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(56232, 5)\n",
      "(12408, 5)\n",
      "(12408, 2)\n",
      "(12408, 5)\n",
      "(12408, 5)\n",
      "(12408, 5)\n",
      "(12408, 4)\n"
     ]
    }
   ],
   "source": [
    "for station in station_data:\n",
    "    print(station_data[station].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79b477",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a36d2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove values which repeat for >= 24 consecutive hours\n",
    "# and values in excess of the monitor's range (> 999.99)\n",
    "def removeBadVals(df, station):\n",
    "    printed = False\n",
    "    ts_index = df.index\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_printed = False\n",
    "        \n",
    "        i = 0\n",
    "        while i + 24 < len(df[col]):\n",
    "            val = df.loc[ts_index[i], col]\n",
    "            # if value is missing\n",
    "            if math.isnan(val):\n",
    "                i += 1\n",
    "                continue\n",
    "            # if value is above the reported range\n",
    "            elif val > 999.99:\n",
    "                df.loc[ts_index[i], col] = math.nan\n",
    "                if not printed:\n",
    "                    print(station)\n",
    "                    print()\n",
    "                    printed = True\n",
    "                if not col_printed:\n",
    "                    print(col + ':')\n",
    "                    col_printed = True\n",
    "                print(str(ts_index[i]) + '\\t\\t\\t\\tExcess Value: ' + str(val))\n",
    "                i += 1\n",
    "            # if values 24-hours apart do not match\n",
    "            elif val != df.loc[ts_index[i+24], col]:\n",
    "                i += 1\n",
    "                continue\n",
    "            # if values 24-hours apart *do* match\n",
    "            else:\n",
    "                j = i\n",
    "                k = i + 24\n",
    "                delete = True\n",
    "                while k >= j:\n",
    "                    # check that all values between indices j and k are the same\n",
    "                    if df.loc[ts_index[j], col] != df.loc[ts_index[k], col]:\n",
    "                        delete = False\n",
    "                        break\n",
    "                    # and that they are the same as the initial value at index i\n",
    "                    elif df.loc[ts_index[j], col] != val:\n",
    "                        delete = False\n",
    "                        break\n",
    "                    j += 1\n",
    "                    k -= 1\n",
    "                # replace all repeated values with NaN, except the first\n",
    "                if delete:\n",
    "                    end = i + 24\n",
    "                    while end < len(df[col]) and val == df.loc[ts_index[end], col]:\n",
    "                        end += 1\n",
    "                    for index in range(i+1, end):\n",
    "                        df.loc[ts_index[index], col] = math.nan\n",
    "                    if not printed:\n",
    "                        print(station)\n",
    "                        print()\n",
    "                        printed = True\n",
    "                    if not col_printed:\n",
    "                        print(col + ':')\n",
    "                        col_printed = True\n",
    "                    print(str(ts_index[i]) + ' | ' + str(ts_index[end]) + '\\tValue: ' + str(val))\n",
    "                    i += end    \n",
    "                else:\n",
    "                    i += 1\n",
    "        # check remaining values to see if they exceed the reported range\n",
    "        while i < len(df[col]):\n",
    "            val = df.loc[ts_index[i], col]\n",
    "            if val > 999.99:\n",
    "                df.loc[ts_index[i], col] = math.nan\n",
    "                if not printed:\n",
    "                    print(station)\n",
    "                    print()\n",
    "                    printed = True\n",
    "                if not col_printed:\n",
    "                    print(col + ':')\n",
    "                    col_printed = True   \n",
    "                print(str(ts_index[i]) + '\\t\\t\\t\\tExcess Value: ' + str(val))\n",
    "            i += 1\n",
    "            \n",
    "    if printed:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f0ae6ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnandVihar\n",
      "\n",
      "PM2.5:\n",
      "2019-03-19 09:00:00 | 2019-03-20 11:00:00\tValue: 182.0\n",
      "PM10:\n",
      "2016-04-09 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-12-31 20:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-09-02 00:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-09-02 01:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2019-11-06 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "\n",
      "AyaNagar\n",
      "\n",
      "PM2.5:\n",
      "2018-03-13 16:00:00 | 2018-03-16 13:00:00\tValue: 8.0\n",
      "NO2:\n",
      "2019-07-22 14:00:00 | 2019-07-25 09:00:00\tValue: 63.83\n",
      "\n",
      "CRRI\n",
      "\n",
      "PM10:\n",
      "2018-04-12 00:00:00\t\t\t\tExcess Value: 1000.0\n",
      "NO2:\n",
      "2018-02-14 19:00:00 | 2018-02-16 15:00:00\tValue: 24.0\n",
      "2021-04-04 00:00:00 | 2021-04-06 00:00:00\tValue: 178.1\n",
      "Ozone:\n",
      "2018-02-14 19:00:00 | 2018-02-16 15:00:00\tValue: 10.0\n",
      "\n",
      "DTU\n",
      "\n",
      "PM2.5:\n",
      "2017-08-11 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-08-11 13:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-08-11 14:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-08-11 15:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-08-11 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-08-11 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-08-11 19:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-08-11 20:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-08-12 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-08-12 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-05-14 13:00:00 | 2018-05-15 18:00:00\tValue: 48.0\n",
      "PM10:\n",
      "2020-10-11 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-10-11 10:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-11-14 22:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-11-14 23:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-11-15 00:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-11-15 01:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-11-15 02:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-04-16 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-05-23 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-05-23 06:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-05-23 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-05-23 08:00:00\t\t\t\tExcess Value: 1000.0\n",
      "\n",
      "IGI\n",
      "\n",
      "PM10:\n",
      "2017-12-14 07:00:00 | 2017-12-16 12:00:00\tValue: 30.0\n",
      "\n",
      "ITO\n",
      "\n",
      "PM2.5:\n",
      "2018-11-16 23:00:00 | 2018-11-18 19:00:00\tValue: 120.0\n",
      "PM10:\n",
      "2018-11-16 23:00:00 | 2018-11-19 10:00:00\tValue: 173.0\n",
      "NO2:\n",
      "2015-10-24 21:00:00 | 2015-10-26 05:00:00\tValue: 6.0\n",
      "2018-11-16 23:00:00 | 2018-11-19 10:00:00\tValue: 66.6\n",
      "SO2:\n",
      "2018-11-16 23:00:00 | 2018-11-19 10:00:00\tValue: 2.61\n",
      "\n",
      "Jahangirpuri\n",
      "\n",
      "PM2.5:\n",
      "2019-10-28 00:00:00\t\t\t\tExcess Value: 1000.0\n",
      "\n",
      "Mundka\n",
      "\n",
      "PM2.5:\n",
      "2020-09-11 23:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-10-11 00:00:00\t\t\t\tExcess Value: 1000.0\n",
      "\n",
      "Narela\n",
      "\n",
      "PM10:\n",
      "2019-04-11 04:00:00\t\t\t\tExcess Value: 1000.0\n",
      "\n",
      "NehruNagar\n",
      "\n",
      "PM2.5:\n",
      "2019-01-16 21:00:00\t\t\t\tExcess Value: 1000.0\n",
      "PM10:\n",
      "2019-01-16 22:00:00\t\t\t\tExcess Value: 1000.0\n",
      "\n",
      "NorthCampus\n",
      "\n",
      "PM2.5:\n",
      "2017-12-23 20:00:00 | 2017-12-26 14:00:00\tValue: 137.79\n",
      "PM10:\n",
      "2017-12-23 20:00:00 | 2017-12-26 14:00:00\tValue: 186.08\n",
      "NO2:\n",
      "2018-02-05 22:00:00 | 2018-05-05 13:00:00\tValue: 22.72\n",
      "Ozone:\n",
      "2017-12-23 20:00:00 | 2017-12-26 14:00:00\tValue: 26.64\n",
      "2021-03-23 19:00:00 | 2021-04-01 00:00:00\tValue: 10.29\n",
      "\n",
      "NSITDwarka\n",
      "\n",
      "PM2.5:\n",
      "2016-08-07 12:00:00 | 2016-10-07 13:00:00\tValue: 104.9\n",
      "2019-03-06 15:00:00 | 2019-04-06 23:00:00\tValue: 124.02\n",
      "SO2:\n",
      "2018-02-07 09:00:00 | 2018-03-07 10:00:00\tValue: 2.3\n",
      "\n",
      "RKPuram\n",
      "\n",
      "PM10:\n",
      "2015-09-13 09:00:00 | 2015-09-14 17:00:00\tValue: 265.0\n",
      "2021-04-17 08:00:00 | 2021-04-19 04:00:00\tValue: 241.0\n",
      "\n",
      "Sirifort\n",
      "\n",
      "PM2.5:\n",
      "2016-06-11 13:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-06-11 20:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-06-11 21:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-07-11 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-16 11:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-16 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 03:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 06:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 08:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 09:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 10:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 11:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 13:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 14:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-17 19:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-18 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-19 11:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-19 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-19 13:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-19 20:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-20 04:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-20 13:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-21 02:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-21 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-21 10:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-21 11:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-21 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-21 15:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-21 21:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-21 22:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-22 03:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-22 04:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-22 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-22 06:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-22 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-22 08:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-22 09:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-22 10:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-22 11:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-22 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-22 13:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2016-11-22 14:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-13 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-14 03:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-15 22:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-17 13:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-17 15:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-19 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-19 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-19 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-20 11:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-20 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-20 23:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-21 03:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-21 04:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-21 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-21 06:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-21 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-21 08:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-21 11:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-21 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-21 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-24 10:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-01-24 11:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-14 22:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-14 23:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-16 13:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-23 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-23 19:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-23 22:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 03:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 04:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 06:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 08:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 09:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 10:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 11:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 13:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 19:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 20:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-24 21:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 02:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 03:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 04:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 06:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 11:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 13:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 14:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 15:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 20:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 21:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 22:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-25 23:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 00:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 01:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 08:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 09:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 10:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 13:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 14:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 15:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 19:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 20:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 21:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 22:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-26 23:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-27 00:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-27 01:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-27 02:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-27 03:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-27 04:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-27 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-27 06:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-27 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-09-04 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-10-04 06:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-10-04 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-11-04 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-04-17 13:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-02-05 15:00:00\t\t\t\tExcess Value: 1000.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-20 01:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-10-20 02:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-10-20 03:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-10-20 04:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-10-20 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-10-20 08:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-10-20 09:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-10-20 10:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2017-10-11 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-12-01 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-01-31 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-03-21 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-03-23 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-03-31 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-04 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-04 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-04-16 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-04-17 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-02-05 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-05-16 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-05-16 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-05-28 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-01-06 14:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-03-07 14:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-01-08 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-07-11 23:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-08-11 00:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-08-11 01:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-08-11 02:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-12-16 22:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-12-29 15:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-12-29 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "PM10:\n",
      "2018-04-16 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-04-17 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-04-17 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-04-20 20:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-04-21 02:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-01-05 10:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-02-05 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-05-16 16:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-05-16 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-01-06 14:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-13 17:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-13 18:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-14 03:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-14 04:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-14 09:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-14 10:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-15 00:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-15 01:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-15 02:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-15 03:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-15 04:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-15 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-15 06:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-15 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-06-29 20:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2019-04-01 10:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2019-04-01 11:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2019-04-01 12:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-11-14 23:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-11-15 00:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-11-15 01:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-11-15 02:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-01-03 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-02-18 08:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-05-03 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-05-23 05:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-05-23 06:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-05-23 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-05-23 08:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2021-05-23 09:00:00\t\t\t\tExcess Value: 1000.0\n",
      "\n",
      "SoniaVihar\n",
      "\n",
      "PM10:\n",
      "2019-03-11 07:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2019-03-11 08:00:00\t\t\t\tExcess Value: 1000.0\n",
      "\n",
      "VivekVihar\n",
      "\n",
      "PM10:\n",
      "2018-12-23 23:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2018-12-24 00:00:00\t\t\t\tExcess Value: 1000.0\n",
      "2020-11-15 02:00:00\t\t\t\tExcess Value: 1000.0\n",
      "Ozone:\n",
      "2019-04-10 14:00:00 | 2019-05-10 16:00:00\tValue: 9.8\n",
      "\n",
      "Wazirpur\n",
      "\n",
      "PM10:\n",
      "2018-12-24 02:00:00\t\t\t\tExcess Value: 1000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for station in station_data:\n",
    "    removeBadVals(station_data[station], station)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b05fd3",
   "metadata": {},
   "source": [
    "# Resave data for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8b09ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in station_data:\n",
    "    station_data[station].to_csv('station_data/data_' + station + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
